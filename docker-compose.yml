version: '3.8'
services:
  llamaserver:
    # image: vllm/vllm-openai:latest
    # image: qwenllm/qwenvl:2-cu121
    image: vllm-local:latest
    working_dir: /vllm-workspace
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [ "gpu" ]
    ports:
      - "8002:8000"
    environment:
      GPU_VRAM: "9GB"
      CPU_RAM: "30GB"
      APPID: "neibudia"
      APPKEY: "26c063eeabcd3d49cc6884b12ade280e"
      EMBEDDING_URL: "http://embeddingserver-app-1:8000/v1/embeddings"
      ENABLE_CHUNKED_PREFILL: "false"
      ENABLE_SPECULATIVE_DECODING: "true"
      # LOGIT_BIAS: '{"151645":10.0}'
    volumes:
      # - /home/huangweiwen/Documents/PythonProjects/LlamaServer/llama_weights/Qwen2___5-7B-Instruct-GPTQ-Int4:/vllm-workspace/model
      - /home/huangweiwen/Documents/PythonProjects/LlamaServer/llama_weights/Qwen2-VL-7B-Instruct-GPTQ-Int4:/vllm-workspace/model
      - /home/huangweiwen/Documents/PythonProjects/LlamaServer/llama_weights/Qwen2-0.5B-Instruct-GPTQ-Int4:/workspace/draft_model
    restart: unless-stopped
    ipc: host
    entrypoint: [ "python3", "-m", "vllm.entrypoints.openai.api_server" ]
    command: --model model --served-model-name 'Qwen2-VL-7B' --trust-remote-code --enforce-eager -tp 1 --max-model-len 6144 --gpu-memory-utilization 0.8
    # command: --model model --served-model-name 'Qwen2-7B(100k)' --trust-remote-code --enforce-eager -tp 1 --max-model-len 6144 --limit-mm-per-prompt image=2 --gpu-memory-utilization 0.8 --enable-auto-tool-choice --tool-call-parser hermes #--rope-scaling {"factor":4.0,"original_max_position_embeddings":32768,"type":"yarn"}

    #/usr/local/lib/python3.10/dist-packages/vllm/spec_decode/spec_decode_worker.py
    #python3 -m vllm.entrypoints.openai.api_server
